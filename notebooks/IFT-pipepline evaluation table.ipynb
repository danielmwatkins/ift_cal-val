{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e46c0c6-db20-4725-8019-0363713c797c",
   "metadata": {},
   "source": [
    "# IFT-pipeline evaluation table\n",
    "Now that there is a functional pipeline, we can use it to identify where various stages of the pipeline fail to guide development. This notebook checks the output of each subfolder given a site specification table and a results folder.\n",
    "\n",
    "## Steps that we are checking\n",
    "1. soit\n",
    "2. landmask\n",
    "3. preprocess\n",
    "4. extractfeatures\n",
    "5. tracking\n",
    "6. exportH5\n",
    "\n",
    "The first four are applied linearly, so the maximum number of successes for step `i` is the number of successes at step `i-1`. Here, we are considering a success if the task was completed with error, i.e., *something* is in the appropriate folder.\n",
    "\n",
    "TBD: Consider replacing this notebook with a section in the README and a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d6df268-8859-444b-8c02-4264a2a9ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95d0724e-197f-49b0-a28b-948348f08332",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {'baffin_bay': [],\n",
    "        'beaufort_sea': [],\n",
    "        'barents-kara_seas': [],\n",
    "        'chukchi-east_siberian_sea': [],\n",
    "        'greenland_sea': [],\n",
    "        'hudson_bay': [],\n",
    "        'laptev_sea': [],\n",
    "        'sea_of_okhostk': []}\n",
    "for region in runs:\n",
    "    fnames = os.listdir('../data/ift_results/' + region)\n",
    "    for file in fnames:\n",
    "        if file[0] != '.':\n",
    "            runs[region].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9a5fb96-0a4f-4bd0-ae64-19657d63c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for region in runs:\n",
    "    results[region] = {}\n",
    "    for timestamp in runs[region]:   \n",
    "        site_locations = pd.read_csv('../data/ift_case_definitions/' + region + '_100km_cases.csv', index_col='location')\n",
    "        results_folder = region + '/' + timestamp\n",
    "        results_loc = '../data/ift_results/' + results_folder\n",
    "    \n",
    "        # soit successes\n",
    "        site_locations['soit'] = 'NA'\n",
    "        for case in site_locations.index:\n",
    "            if 'soit' in os.listdir(results_loc + '/' + case):\n",
    "                if len(os.listdir(results_loc + '/' + case + '/soit' )) > 0:\n",
    "                    site_locations.loc[case, 'soit'] = 'pass'\n",
    "                else:\n",
    "                    site_locations.loc[case, 'soit'] = 'fail'\n",
    "        \n",
    "        # landmask successes\n",
    "        site_locations['landmask'] = 'NA'\n",
    "        for case in site_locations.index:\n",
    "            files = [x for x in os.listdir(results_loc + '/' + case + '/landmasks/') if x != '.DS_Store']\n",
    "            if len(files) != 0:\n",
    "                site_locations.loc[case, 'landmask'] = 'pass'\n",
    "            elif site_locations.loc[case, 'soit'] == 'pass':\n",
    "                site_locations.loc[case, 'landmask'] = 'fail'\n",
    "        \n",
    "        # preprocessing successes\n",
    "        # here, slightly different check. hdf5-files will always be there.\n",
    "        site_locations['preprocess'] = 'NA'\n",
    "        site_locations['extractH5'] = 'NA'\n",
    "        site_locations['tracker'] = 'NA'\n",
    "        for case in site_locations.index:\n",
    "            files = [x for x in os.listdir(results_loc + '/' + case + '/preprocess/') if x not in ['.DS_Store', 'hdf5-files']]\n",
    "            if len(files) != 0:\n",
    "                site_locations.loc[case, 'preprocess'] = 'pass'\n",
    "                h5files = [x for x in os.listdir(results_loc + '/' + case + '/preprocess/hdf5-files') if x != '.DS_Store']\n",
    "        \n",
    "                # Check h5 and tracker if it passes the preprocess step\n",
    "                if len(h5files) != 0:\n",
    "                    site_locations.loc[case, 'extractH5'] = 'pass'\n",
    "                else:\n",
    "                    site_locations.loc[case, 'extractH5'] = 'fail'\n",
    "                trfiles = [x for x in os.listdir(results_loc + '/' + case + '/tracker') if x != '.DS_Store']            \n",
    "                if len(trfiles) != 0:\n",
    "                    site_locations.loc[case, 'tracker'] = 'pass'\n",
    "                else:\n",
    "                    site_locations.loc[case, 'tracker'] = 'fail'            \n",
    "        \n",
    "            elif site_locations.loc[case, 'soit'] == 'pass':\n",
    "                if site_locations.loc[case, 'landmask'] == 'pass': \n",
    "                    site_locations.loc[case, 'preprocess'] = 'fail'\n",
    "        \n",
    "        site_locations.loc[:,['soit', 'landmask', 'preprocess', 'extractH5', 'tracker']].to_csv(\n",
    "            results_loc + '/' + region + '_evaluation_table.csv')\n",
    "        results[region][timestamp] = site_locations.loc[:,['soit', 'landmask', 'preprocess', 'extractH5', 'tracker']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d679ad39-cfc8-4607-87c9-fa370563c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempted = {}\n",
    "for region in results:\n",
    "    attempted[region] = {}\n",
    "    for timestamp in results[region]:\n",
    "        attempted[region][timestamp] = pd.concat([\n",
    "            (results[region][timestamp].loc[:,['soit', 'landmask', 'preprocess', 'extractH5', 'tracker']] == 'pass').sum(axis=0),\n",
    "            (results[region][timestamp].loc[:,['soit', 'landmask', 'preprocess', 'extractH5', 'tracker']] == 'fail').sum(axis=0)], axis=1)\n",
    "        attempted[region][timestamp].columns = ['pass', 'fail']\n",
    "        attempted[region][timestamp]['attempted'] = attempted[region][timestamp]['pass'] + attempted[region][timestamp]['fail']\n",
    "# attempted = pd.concat(attempted) \n",
    "# attempted = attempted.reset_index()\n",
    "# attempted.rename({'level_0': 'region','level_1': 'task'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0413f051-5460-4cc1-9811-9984f95c551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of failed attempts\n"
     ]
    }
   ],
   "source": [
    "order = ['soit', 'landmask', 'preprocess', 'extractH5', 'tracker']\n",
    "attempted['fail_fraction'] = attempted['fail'] / attempted['attempted']\n",
    "attempted.pivot_table(index='region', columns='task', values='fail_fraction').loc[:, order].round(2)\n",
    "print('Fraction of failed attempts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a0da42b-4ec0-4354-b4c6-b3ce20807d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            pass  fail  attempted\n",
      "soit          19     2         21\n",
      "landmask      19     0         19\n",
      "preprocess     2    17         19\n",
      "extractH5      2     0          2\n",
      "tracker        1     1          2\n",
      "            pass  fail  attempted\n",
      "soit          19     2         21\n",
      "landmask      19     0         19\n",
      "preprocess     2    17         19\n",
      "extractH5      2     0          2\n",
      "tracker        1     1          2\n"
     ]
    }
   ],
   "source": [
    "region = 'chukchi-east_siberian_sea'\n",
    "for timestamp in attempted[region]:\n",
    "    print(attempted[region][timestamp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0038a342-6808-4684-8224-9dfd2bdb0e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-10-31',\n",
       " '2019-11-30',\n",
       " '2019-12-31',\n",
       " '2020-01-31',\n",
       " '2020-02-29',\n",
       " '2020-03-31',\n",
       " '2020-04-30',\n",
       " '2020-05-31',\n",
       " '2020-06-30',\n",
       " '2020-07-31',\n",
       " '2020-08-31',\n",
       " '2020-09-30',\n",
       " '2020-10-31',\n",
       " '2020-11-30',\n",
       " '2020-12-31',\n",
       " '2021-01-31',\n",
       " '2021-02-28',\n",
       " '2021-03-31',\n",
       " '2021-04-30']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_dates = [m.strftime('%Y-%m-%d') for m in pd.date_range('2019-10-01', '2021-05-06', freq='1MS')]\n",
    "end_dates = [m.strftime('%Y-%m-%d') for m in pd.date_range('2019-10-01', '2021-05-06', freq='1ME')]\n",
    "end_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc7fd8-a430-402a-95e9-6a845471b24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
